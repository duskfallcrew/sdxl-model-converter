{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dY-0HMB7VyNu"
   },
   "source": [
    "# **SDXL Model Converter Jupyter Edition**\n",
    "A Colab Notebook To Convert SDXL Checkpoint to Diffusers format\n",
    "Originally coded by [Linaqruf](https://github.com/Linaqruf?tab=repositories)\n",
    "\n",
    "Ported from Linaqruf's SDXL one, and added extra functionality from the SD 1.5 converter.\n",
    "\n",
    "#### Summary:\n",
    "\n",
    "This notebook is a comprehensive tool for converting and uploading Diffusers models to the Hugging Face Hub. It provides a user-friendly interface for:\n",
    "\n",
    "- Converting Diffusers models to the Hugging Face format\n",
    "- Selecting and uploading folders containing the converted models to the Hugging Face Hub\n",
    "- Authenticating with the Hugging Face Hub using the notebook_login() function\n",
    "- Specifying repository details, including the Hugging Face username, repository name, and organization name (if applicable)\n",
    "- Choosing upload options, such as making the repository private or not\n",
    "- Monitoring upload progress and verifying the success of the upload\n",
    "- Cleaning folders\n",
    "- Fixing SDXL Keys.\n",
    "\n",
    "This is the JUPYTER edition that i'm working on, please note this will be largely incompatible with the colab runtime. \n",
    "Instructions are missing at the moment because i can't brain, and nor can Llama3b.\n",
    "\n",
    "----------------\n",
    "### **Important Note:**\n",
    "\n",
    "\n",
    "Before diving in, ensure you create a Hugging Face token with write permissions. Follow this link for instructions on token creation.\n",
    "\n",
    "You need to create a huggingface token, go to [this link](https://huggingface.co/settings/tokens), then `create new token` or copy available token with the `Write` role.\n",
    "\n",
    "-----------------\n",
    "\n",
    "\n",
    "| Link Name| Description | Link |\n",
    "| --- | --- | --- |\n",
    "| [Huggingface Backup](https://colab.research.google.com/github/kieranxsomer/HuggingFace_Backup/blob/main/HuggingFace_Backup.ipynb) | backup checkpoints! | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=flat)](https://colab.research.google.com/github/kieranxsomer/HuggingFace_Backup/blob/main/HuggingFace_Backup.ipynb)\n",
    "| [SD 1.5 Conversion to Diffusers](https://colab.research.google.com/drive/1zAzdsaa2KQcF6W0V4eCLZ6eUO8hsDJTo?usp=drive_link)| Convert SD 1.5 to Diffusers| [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=flat)](https://colab.research.google.com/drive/1zAzdsaa2KQcF6W0V4eCLZ6eUO8hsDJTo?usp=drive_link)\n",
    "| [SDXL Conversion to Diffusers](https://colab.research.google.com/drive/1CcSCmUB_UkT-8TlUkwDDKnHB4T7nti01?usp=drive_link)| Convert SDXL to Diffusers| [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=flat)](https://colab.research.google.com/drive/1CcSCmUB_UkT-8TlUkwDDKnHB4T7nti01?usp=drive_link)\n",
    "|Discord| E&D Discord |[Invite](https://discord.gg/5t2kYxt7An)\n",
    "|CivitAi| Duskfallcrew @ Civitai |[Duskfallcrew](https://civitai.com/user/duskfallcrew/)\n",
    "|Huggingface| E&D Huggingface |[Earth & Dusk](https://huggingface.co/EarthnDusk)\n",
    "|Ko-Fi| Kofi Support |[![ko-fi](https://img.shields.io/badge/Support%20me%20on%20Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat)](https://ko-fi.com/Z8Z8L4EO)\n",
    "|Github| Duskfallcrew Github |[Duskfallcrew](https://github.com/duskfallcrew)\n",
    "| Youtube: | Duskfall Music|[Duskfall Music & More](https://www.youtube.com/channel/UCk7MGP7nrJz5awBSP75xmVw)\n",
    "| Spotify: | E&D Royalty Free| [PLAYLIST](https://open.spotify.com/playlist/00R8x00YktB4u541imdSSf?si=57a8f0f0fe87434e)\n",
    "|DA Group | AI Group| [DeviantArt Group](https://www.deviantart.com/diffusionai)\n",
    "| Reddit | Earth & Dusk| [Subreddit](https://www.reddit.com/r/earthndusk/)\n",
    "\n",
    "\n",
    "\n",
    "> ## Collaboration\n",
    "\n",
    "\n",
    "I am NOT A programmer by nature, I patch with what little knowledge I have. I Failed programming several times over the years, so if something needs cleaning up and you want to patch it - pull request it!\n",
    "\n",
    "\n",
    ">## About\n",
    "\n",
    "\n",
    "We are a system of over 300 alters, proudly navigating life with Dissociative Identity Disorder, ADHD, Autism, and CPTSD. We believe in the potential of AI to break down barriers and enhance aspects of mental health, even as it presents challenges. Our creative journey is an ongoing exploration of identity and expression, and we invite you to join us in this adventure.\n",
    "\n",
    "\n",
    "\n",
    ">Future ideas:\n",
    "\n",
    "- Looking to port PT to safetensors into the same notebook.\n",
    "- Looking to figure out how to manage to get the conversions to roll on google drive for more space options.\n",
    "- Porting it to VastAi/Runpod\n",
    "- Looking to port inference for testing into the same notebook. - Not sure why you'd need this unless you're training, I think this was in the SDXL one from Linaqruf, but this isn't required on this one.\n",
    "- Looking to figure out how to convert a different vae. - I could NOT get this to work no matter what I did. I have some THEORIES, but i'm not a programmer. However due to the fact I have a SCRIPT for this now - this may be an option.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ">## Credits:\n",
    "\n",
    "\n",
    "| Patched Origin | Description | Link |\n",
    "| --- | --- | --- |\n",
    "|Patched from| ARCHIVED |[SDXL - Linaqruf](https://colab.research.google.com/github/Linaqruf/sdxl-model-converter/blob/main/sdxl_model_converter.ipynb)\n",
    "|***Linaqruf @ Github***: |https://github.com/Linaqruf\n",
    "|Linaqruf Ko-Fi | [![](https://dcbadge.vercel.app/api/shield/850007095775723532?style=flat)](https://lookup.guru/850007095775723532) [![ko-fi](https://img.shields.io/badge/Support%20me%20on%20Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat)](https://ko-fi.com/linaqruf)\n",
    "| Linaqruf Saweria |<a href=\"https://saweria.co/linaqruf\"><img alt=\"Saweria\" src=\"https://img.shields.io/badge/Saweria-7B3F00?style=flat&logo=ko-fi&logoColor=white\"/></a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ‚ôª **Install Diffusers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "R3hfwYaXKn_V"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import getoutput\n",
    "\n",
    "# Directories\n",
    "root_dir = \"/path/to/your/workspace\"  # Update this to your workspace directory\n",
    "models_dir = os.path.join(root_dir, \"models\")\n",
    "vae_dir = os.path.join(root_dir, \"vae\")\n",
    "\n",
    "# Setup Directories\n",
    "def setup_directories(dirs):\n",
    "    for dir in dirs:\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def install_dependencies():\n",
    "    !pip install -q  torch==2.1.0+cu121 diffusers[torch]==0.25.0 transformers==4.36.0 einops==0.6.0 open-clip-torch==2.20.0 invisible-watermark  xformers==0.0.23  jax[cuda12_pip]==0.4.23  -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html ipywidgets\n",
    "    !pip install huggingface_hub --upgrade\n",
    "\n",
    "def prepare_environment():\n",
    "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "    os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.9,max_split_size_mb:512\"\n",
    "    os.environ[\"TCMALLOC_AGGRESSIVE_DECOMMIT\"] = \"t\"\n",
    "    os.environ[\"CUDA_MODULE_LOADING\"] = \"LAZY\"\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.chdir(root_dir)\n",
    "    setup_directories([models_dir, vae_dir])\n",
    "    install_dependencies()\n",
    "    prepare_environment()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ôª **Clean Folder**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "sRiX3jnynw3P"
   },
   "outputs": [],
   "source": [
    "from ipywidgets import Text, Button, Output\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Function to clear and delete a folder\n",
    "def clear_and_delete_folder(folder_path):\n",
    "    try:\n",
    "        # Use shutil.rmtree to remove all files and subdirectories\n",
    "        shutil.rmtree(folder_path)\n",
    "        display(Markdown(f\"Deleted all contents in folder: `{folder_path}`\"))\n",
    "    except Exception as e:\n",
    "        display(Markdown(f\"Error deleting folder `{folder_path}`: {e}\"))\n",
    "\n",
    "# Create a text input widget for the folder path\n",
    "folder_path_input = Text(value=\"\", placeholder=\"Enter folder path\", description=\"Folder Path:\")\n",
    "\n",
    "# Create a button widget to trigger the deletion\n",
    "delete_button = Button(description=\"Delete Folder\")\n",
    "\n",
    "# Create an output widget to display the result\n",
    "output_widget = Output()\n",
    "\n",
    "# Define a function to handle the button click event\n",
    "def delete_folder(b):\n",
    "    folder_path = folder_path_input.value\n",
    "    with output_widget:\n",
    "        clear_and_delete_folder(folder_path)\n",
    "\n",
    "# Link the button click event to the delete_folder function\n",
    "delete_button.on_click(delete_folder)\n",
    "\n",
    "\n",
    "# Add a clear warning to users\n",
    "display(Markdown(\"\"\"\n",
    " **‚ö†Ô∏è WARNING:** \n",
    " Be very careful when using this function as it will **PERMANENTLY DELETE** the specified folder and **all of its contents**. This operation cannot be undone.\n",
    " \"\"\"))\n",
    "\n",
    "# Display the widgets\n",
    "display(folder_path_input)\n",
    "display(delete_button)\n",
    "display(output_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ôª **Fix Before Converting (Optional, may not work)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "NiWfPWulD6rW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import torch\n",
    "from IPython.display import display, Markdown\n",
    "from ipywidgets import Text, Button, Output, Select\n",
    "\n",
    "# Note that this is meant to work IN TANDEM with the colab notebook, you'll need the installation of kohya-ss FIRST. in tandem meaning this is just the PY file that works seperate to the colab, and this works just as wel las the other.\n",
    "\n",
    "def fix_sdxl_model_keys(model_data):\n",
    "    text_encoder1, text_encoder2, vae, unet = model_data\n",
    "\n",
    "    # Fix keys in text_encoder1\n",
    "    fixed_text_encoder1 = {}\n",
    "    for k, v in text_encoder1.state_dict().items():\n",
    "        new_k = k.replace('first_stage_model.', '')\n",
    "        fixed_text_encoder1[new_k] = v\n",
    "\n",
    "    # Fix keys in text_encoder2\n",
    "    fixed_text_encoder2 = {}\n",
    "    for k, v in text_encoder2.state_dict().items():\n",
    "        new_k = k.replace('first_stage_model.', '')\n",
    "        fixed_text_encoder2[new_k] = v\n",
    "\n",
    "    # Fix keys in vae\n",
    "    fixed_vae = {}\n",
    "    for k, v in vae.state_dict().items():\n",
    "        new_k = k.replace('first_stage_model.', '')\n",
    "        fixed_vae[new_k] = v\n",
    "\n",
    "    # Fix keys in unet\n",
    "    fixed_unet = {}\n",
    "    for k, v in unet.state_dict().items():\n",
    "        new_k = k.replace('first_stage_model.', '')\n",
    "        fixed_unet[new_k] = v\n",
    "\n",
    "    return fixed_text_encoder1, fixed_text_encoder2, fixed_vae, fixed_unet\n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"\n",
    "    Loads a model from a safetensors or checkpoint file\n",
    "    \"\"\"\n",
    "    load_dtype = torch.float16\n",
    "    if model_path.endswith(\".safetensors\"):\n",
    "        from safetensors import safe_open\n",
    "        model_data = {}\n",
    "        with safe_open(model_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for key in f.keys():\n",
    "                model_data[key] = f.get_tensor(key)\n",
    "    else:\n",
    "        model_data = torch.load(model_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "\n",
    "    # Create dummy model objects\n",
    "    class DummyModel:\n",
    "      def __init__(self, state_dict):\n",
    "          self.state_dict = lambda: state_dict\n",
    "\n",
    "    text_encoder1 = DummyModel( {k:v for k,v in model_data.items() if \"text_encoder.model.\" in k})\n",
    "    text_encoder2 = DummyModel( {k:v for k,v in model_data.items() if \"text_encoder_2.model.\" in k})\n",
    "    vae          = DummyModel( {k:v for k,v in model_data.items() if \"first_stage_model\" in k})\n",
    "    unet         = DummyModel( {k:v for k,v in model_data.items() if \"unet.\" in k})\n",
    "\n",
    "    return text_encoder1, text_encoder2, vae, unet\n",
    "\n",
    "def apply_key_fix(model_path, output_widget):\n",
    "    \"\"\"\n",
    "    Loads a model and applies the key fix, then it displays a success message\n",
    "    \"\"\"\n",
    "    with output_widget:\n",
    "        try:\n",
    "            loaded_model_data = load_model(model_path)\n",
    "            fixed_model_data = fix_sdxl_model_keys(loaded_model_data)\n",
    "            print(\"Keys have been fixed and are loaded in memory.\")\n",
    "            return fixed_model_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error fixing model keys: {e}\")\n",
    "            return None\n",
    "\n",
    "model_select = Select(\n",
    "    options=[\"\", \"sdxl-base\"],\n",
    "    value=\"\",\n",
    "    description=\"Select model type\"\n",
    ")\n",
    "# Create a text input widget for the folder path\n",
    "model_path_input = Text(value=\"\", placeholder=\"Enter Model Path\", description=\"Model Path:\")\n",
    "\n",
    "# Create a button widget to trigger the download\n",
    "fix_button = Button(description=\"Fix Keys\")\n",
    "\n",
    "# Create an output widget to display the results\n",
    "output_widget = Output()\n",
    "\n",
    "# Define a function to handle the button click event\n",
    "def fix_model(b):\n",
    "    model_path = model_path_input.value\n",
    "    apply_key_fix(model_path, output_widget)\n",
    "\n",
    "\n",
    "# Link the button click event to the delete_folder function\n",
    "fix_button.on_click(fix_model)\n",
    "\n",
    "# Display the widgets\n",
    "display(model_select)\n",
    "display(model_path_input)\n",
    "display(fix_button)\n",
    "display(output_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ‚ôª  **Download SDXL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "_nmd5ciZKv3Z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import gdown\n",
    "import requests\n",
    "import subprocess\n",
    "from urllib.parse import urlparse, unquote\n",
    "from pathlib import Path\n",
    "from ipywidgets import Text, Button, Output\n",
    "\n",
    "# Create text widgets for input\n",
    "huggingface_token_widget = Text(value=\"\", placeholder=\"Huggingface Read Token\", description=\"Huggingface Token:\")\n",
    "sdxl_model_url_widget = Text(value=\"\", placeholder=\"SDXL Model URL\", description=\"SDXL Model URL:\")\n",
    "download_path_widget = Text(value=os.getcwd(), placeholder=\"Download Path\", description=\"Download Path:\")\n",
    "\n",
    "\n",
    "# Create a button to trigger the download\n",
    "download_button = Button(description=\"Download SDXL Model\")\n",
    "\n",
    "# Create an output widget to display the results\n",
    "output_widget = Output()\n",
    "\n",
    "def get_supported_extensions():\n",
    "    return tuple([\".ckpt\", \".safetensors\", \".pt\", \".pth\"])\n",
    "\n",
    "\n",
    "def get_filename(url):\n",
    "    extensions = get_supported_extensions()\n",
    "\n",
    "    if url.endswith(tuple(extensions)):\n",
    "        filename = os.path.basename(url)\n",
    "    else:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        if 'content-disposition' in response.headers:\n",
    "            content_disposition = response.headers['content-disposition']\n",
    "            filename = re.findall('filename=\"?([^\"]+)\"?', content_disposition)[0]\n",
    "        else:\n",
    "            url_path = urlparse(url).path\n",
    "            filename = unquote(os.path.basename(url_path))\n",
    "\n",
    "    if filename.endswith(tuple(get_supported_extensions())):\n",
    "        return filename\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def parse_args(config):\n",
    "    args = []\n",
    "\n",
    "    for k, v in config.items():\n",
    "        if k.startswith(\"_\"):\n",
    "            args.append(f\"{v}\")\n",
    "        elif isinstance(v, str) and v is not None:\n",
    "            args.append(f'--{k}={v}')\n",
    "        elif isinstance(v, bool) and v:\n",
    "            args.append(f\"--{k}\")\n",
    "        elif isinstance(v, float) and not isinstance(v, bool):\n",
    "            args.append(f\"--{k}={v}\")\n",
    "        elif isinstance(v, int) and not isinstance(v, bool):\n",
    "            args.append(f\"--{k}={v}\")\n",
    "\n",
    "    return args\n",
    "\n",
    "def aria2_download(dir, filename, url, huggingface_token):\n",
    "    user_header = f\"Authorization: Bearer {huggingface_token}\"\n",
    "\n",
    "    aria2_config = {\n",
    "        \"console-log-level\"         : \"error\",\n",
    "        \"summary-interval\"          : 10,\n",
    "        \"header\"                    : user_header if \"huggingface.co\" in url else None,\n",
    "        \"continue\"                  : True,\n",
    "        \"max-connection-per-server\" : 16,\n",
    "        \"min-split-size\"            : \"1M\",\n",
    "        \"split\"                     : 16,\n",
    "        \"dir\"                       : dir,\n",
    "        \"out\"                       : filename,\n",
    "        \"_url\"                      : url,\n",
    "    }\n",
    "    aria2_args = parse_args(aria2_config)\n",
    "    subprocess.run([\"aria2c\", *aria2_args])\n",
    "\n",
    "def gdown_download(url, dst, filepath):\n",
    "    if \"/uc?id/\" in url or \"/file/d/\" in url:\n",
    "        return gdown.download(url, filepath, quiet=False)\n",
    "    elif \"/drive/folders/\" in url:\n",
    "        os.chdir(dst)\n",
    "        return gdown.download_folder(url, quiet=True, use_cookies=False)\n",
    "\n",
    "def download(url, dst, huggingface_token):\n",
    "    filename = get_filename(url)\n",
    "    filepath = os.path.join(dst, filename)\n",
    "\n",
    "    try:\n",
    "        if \"drive.google.com\" in url:\n",
    "           gdown = gdown_download(url, dst, filepath)\n",
    "        elif url.startswith(\"/content/drive/MyDrive/\"):\n",
    "            return url\n",
    "        else:\n",
    "            if \"huggingface.co\" in url:\n",
    "                if \"/blob/\" in url:\n",
    "                    url = url.replace(\"/blob/\", \"/resolve/\")\n",
    "            aria2_download(dst, filename, url, huggingface_token)\n",
    "    except Exception as e:\n",
    "      with output_widget:\n",
    "          print(f\"Error downloading model: {e}\")\n",
    "\n",
    "\n",
    "def main(b):\n",
    "    huggingface_token = huggingface_token_widget.value\n",
    "    sdxl_model_url = sdxl_model_url_widget.value\n",
    "    download_path = download_path_widget.value\n",
    "\n",
    "    try:\n",
    "        download(sdxl_model_url, download_path, huggingface_token)\n",
    "        with output_widget:\n",
    "          print(f\"Model downloaded to: {download_path}\")\n",
    "    except Exception as e:\n",
    "        with output_widget:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Link the button to the main function\n",
    "download_button.on_click(main)\n",
    "\n",
    "# Display the widgets\n",
    "display(huggingface_token_widget)\n",
    "display(sdxl_model_url_widget)\n",
    "display(download_path_widget)\n",
    "display(download_button)\n",
    "display(output_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ‚ôª **Convert SDXL to Diffusers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "uSEEJiDaK3Qw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from ipywidgets import Text, Button, Output\n",
    "\n",
    "# Create text widgets for input\n",
    "model_to_load_widget = Text(value=\"\", placeholder=\"Model to load\", description=\"Model to load:\")\n",
    "save_precision_as_widget = Text(value=\"fp16\", placeholder=\"Save precision as\", description=\"Save precision as:\")\n",
    "reference_model_widget = Text(value=\"stabilityai/stable-diffusion-xl-base-1.0\", placeholder=\"Reference model\", description=\"Reference model:\")\n",
    "epoch_widget = Text(value=\"0\", placeholder=\"Epoch\", description=\"Epoch:\")\n",
    "global_step_widget = Text(value=\"0\", placeholder=\"Global Step\", description=\"Global Step:\")\n",
    "script_path_widget = Text(value=os.getcwd(), placeholder=\"Path to script\", description=\"Path to script\")\n",
    "\n",
    "\n",
    "# Create a button to trigger the conversion\n",
    "convert_button = Button(description=\"Convert SDXL to Diffusers\")\n",
    "\n",
    "# Create an output widget to display the results\n",
    "output_widget = Output()\n",
    "\n",
    "def convert_dict(config):\n",
    "    args = \"\"\n",
    "    for k, v in config.items():\n",
    "        if k.startswith(\"_\"):\n",
    "            args += f'\"{v}\" '\n",
    "        elif isinstance(v, str):\n",
    "            args += f'--{k}=\"{v}\" '\n",
    "        elif isinstance(v, bool) and v:\n",
    "            args += f\"--{k} \"\n",
    "        elif isinstance(v, float) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "        elif isinstance(v, int) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "    return args\n",
    "\n",
    "def run_script(script_path, script_args, output_widget):\n",
    "    try:\n",
    "        !python {script_path} {script_args}\n",
    "        with output_widget:\n",
    "            print(\"Conversion complete!\")\n",
    "    except Exception as e:\n",
    "        with output_widget:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "def main(b):\n",
    "    model_to_load = model_to_load_widget.value\n",
    "    save_precision_as = save_precision_as_widget.value\n",
    "    reference_model = reference_model_widget.value\n",
    "    epoch = epoch_widget.value\n",
    "    global_step = global_step_widget.value\n",
    "    script_path = script_path_widget.value\n",
    "\n",
    "\n",
    "    config = {\n",
    "        \"model_to_load\": model_to_load,\n",
    "        \"save_precision_as\": save_precision_as,\n",
    "        \"epoch\": epoch,\n",
    "        \"global_step\": global_step,\n",
    "        \"reference_model\": reference_model,\n",
    "    }\n",
    "    script_args = convert_dict(config)\n",
    "    run_script(script_path, script_args, output_widget)\n",
    "\n",
    "# Link the button to the main function\n",
    "convert_button.on_click(main)\n",
    "\n",
    "# Display the widgets\n",
    "display(model_to_load_widget)\n",
    "display(save_precision_as_widget)\n",
    "display(reference_model_widget)\n",
    "display(epoch_widget)\n",
    "display(global_step_widget)\n",
    "display(script_path_widget)\n",
    "display(convert_button)\n",
    "display(output_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ‚ôª  Diffusers Deploy & Upload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "KsVa2oLvRwL0"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from huggingface_hub import notebook_login, HfApi, HfHubHTTPError\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "\n",
    "# Authenticate with Hugging Face Hub\n",
    "notebook_login()\n",
    "\n",
    "# Define widgets for user inputs\n",
    "drive_path = Text(value=os.getcwd(), placeholder='Path to upload from', description='Path:')\n",
    "hfuser = Text(placeholder='YourUSERHERE', description='HF User:')\n",
    "hfrepo = Text(placeholder='ModelNameHere', description='HF Repo:')\n",
    "orgs_name = Text(placeholder='Organization name (optional)', description='Organization name:')\n",
    "make_private = Checkbox(value=False, description='Make repository private')\n",
    "commit_message = Text(value=\"Upload with Earth & Dusk Huggingface ü§ó Backup\", placeholder='Commit Message', description='Commit Message:')\n",
    "\n",
    "# Initialize Hugging Face API\n",
    "api = HfApi()\n",
    "\n",
    "# Define a placeholder for validate_repo_id function\n",
    "def validate_repo_id(repo_id):\n",
    "    \"\"\"\n",
    "    This is a placeholder function for validate_repo_id and can be expanded with more sophisticated checks if needed\n",
    "    \"\"\"\n",
    "    if not repo_id:\n",
    "       raise ValueError(\"Repo ID cannot be empty\")\n",
    "    return True\n",
    "\n",
    "\n",
    "# Define function to update folder list\n",
    "def update_folders(b, folder_picker):\n",
    "    path = Path(drive_path.value)\n",
    "    if path.exists() and path.is_dir():\n",
    "        all_folders = [str(f) for f in path.glob(\"*\") if f.is_dir()]\n",
    "        folder_picker.options = all_folders\n",
    "    else:\n",
    "        with out:\n",
    "            print(\"Invalid directory. Please check the path and try again.\")\n",
    "\n",
    "# Define upload function\n",
    "def upload_folders(b, folder_picker, commit_message):\n",
    "    repo_id = f\"{hfuser.value or hfuser.placeholder}/{hfrepo.value or hfrepo.placeholder}\"\n",
    "    if orgs_name.value:\n",
    "        repo_id = f\"{orgs_name.value}/{hfrepo.value or hfrepo.placeholder}\"\n",
    "    try:\n",
    "        validate_repo_id(repo_id)\n",
    "        api.create_repo(repo_id=repo_id, repo_type=\"model\", private=make_private.value)\n",
    "        print(f\"Model repo '{repo_id}' didn't exist, creating repo\")\n",
    "    except HfHubHTTPError as e:\n",
    "        print(f\"Model repo '{repo_id}' exists, skipping create repo\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating repo {e}\")\n",
    "        return\n",
    "\n",
    "    with out:\n",
    "        if folder_picker is None or len(folder_picker.value) < 1:\n",
    "            print(\"Nothing selected for upload, make sure to select one of the folders in the list, or verify there are folders in the specified directory.\")\n",
    "        for folder in folder_picker.value:\n",
    "            try:\n",
    "                print(f\"Uploading to HF: huggingface.co/{repo_id}/{os.path.basename(folder)}\")\n",
    "                api.upload_folder(\n",
    "                    folder_path=folder,\n",
    "                    path_in_repo=os.path.basename(folder),\n",
    "                    repo_id=repo_id,\n",
    "                    commit_message=commit_message\n",
    "                )\n",
    "            except Exception as e:\n",
    "                 print(f\"Error uploading folder {folder}, {e}\")\n",
    "                 return\n",
    "\n",
    "        print(\"DONE\")\n",
    "        print(\"Go to your repo and accept the PRs created to see your files.\")\n",
    "\n",
    "# Bind the functions to buttons\n",
    "update_btn = Button(description='Update Folders')\n",
    "upload_btn = Button(description='Upload')\n",
    "out = Output()\n",
    "\n",
    "# Initialize folder_picker with no options to avoid initial errors.\n",
    "folder_list = []\n",
    "folder_picker = SelectMultiple(options=folder_list, layout=Layout(width=\"600px\"))\n",
    "\n",
    "def handle_update(b):\n",
    "   update_folders(b, folder_picker)\n",
    "\n",
    "def handle_upload(b):\n",
    "   upload_folders(b, folder_picker, commit_message.value)\n",
    "\n",
    "update_btn.on_click(handle_update)\n",
    "upload_btn.on_click(handle_upload)\n",
    "\n",
    "# Create and display the widget layout\n",
    "box = VBox([\n",
    "    drive_path,\n",
    "    HBox([hfuser, hfrepo]),\n",
    "    HBox([orgs_name, make_private]),\n",
    "    commit_message,\n",
    "    update_btn,\n",
    "    folder_picker,\n",
    "    upload_btn,\n",
    "    out\n",
    "])\n",
    "\n",
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ‚ôª Extra: Vae PT to Safetensors (Should work but hey I dunno!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import io\n",
    "import os\n",
    "import requests\n",
    "import torch\n",
    "import yaml\n",
    "from diffusers import AutoencoderKL\n",
    "from diffusers.pipelines.stable_diffusion.convert_from_ckpt import (\n",
    "    assign_to_checkpoint,\n",
    "    conv_attn_to_linear,\n",
    "    create_vae_diffusers_config,\n",
    "    renew_vae_attention_paths,\n",
    "    renew_vae_resnet_paths,\n",
    ")\n",
    "import safetensors\n",
    "import json\n",
    "from ipywidgets import Text, Button, Output\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def custom_convert_ldm_vae_checkpoint(checkpoint, config):\n",
    "    vae_state_dict = checkpoint\n",
    "\n",
    "    new_checkpoint = {}\n",
    "\n",
    "    new_checkpoint[\"encoder.conv_in.weight\"] = vae_state_dict[\"encoder.conv_in.weight\"]\n",
    "    new_checkpoint[\"encoder.conv_in.bias\"] = vae_state_dict[\"encoder.conv_in.bias\"]\n",
    "    new_checkpoint[\"encoder.conv_out.weight\"] = vae_state_dict[\"encoder.conv_out.weight\"]\n",
    "    new_checkpoint[\"encoder.conv_out.bias\"] = vae_state_dict[\"encoder.conv_out.bias\"]\n",
    "    new_checkpoint[\"encoder.conv_norm_out.weight\"] = vae_state_dict[\"encoder.norm_out.weight\"]\n",
    "    new_checkpoint[\"encoder.conv_norm_out.bias\"] = vae_state_dict[\"encoder.norm_out.bias\"]\n",
    "\n",
    "    new_checkpoint[\"decoder.conv_in.weight\"] = vae_state_dict[\"decoder.conv_in.weight\"]\n",
    "    new_checkpoint[\"decoder.conv_in.bias\"] = vae_state_dict[\"decoder.conv_in.bias\"]\n",
    "    new_checkpoint[\"decoder.conv_out.weight\"] = vae_state_dict[\"decoder.conv_out.weight\"]\n",
    "    new_checkpoint[\"decoder.conv_out.bias\"] = vae_state_dict[\"decoder.conv_out.bias\"]\n",
    "    new_checkpoint[\"decoder.conv_norm_out.weight\"] = vae_state_dict[\"decoder.norm_out.weight\"]\n",
    "    new_checkpoint[\"decoder.conv_norm_out.bias\"] = vae_state_dict[\"decoder.norm_out.bias\"]\n",
    "\n",
    "    new_checkpoint[\"quant_conv.weight\"] = vae_state_dict[\"quant_conv.weight\"]\n",
    "    new_checkpoint[\"quant_conv.bias\"] = vae_state_dict[\"quant_conv.bias\"]\n",
    "    new_checkpoint[\"post_quant_conv.weight\"] = vae_state_dict[\"post_quant_conv.weight\"]\n",
    "    new_checkpoint[\"post_quant_conv.bias\"] = vae_state_dict[\"post_quant_conv.bias\"]\n",
    "\n",
    "    # Retrieves the keys for the encoder down blocks only\n",
    "    num_down_blocks = len({\".\".join(layer.split(\".\")[:3]) for layer in vae_state_dict if \"encoder.down\" in layer})\n",
    "    down_blocks = {\n",
    "        layer_id: [key for key in vae_state_dict if f\"down.{layer_id}\" in key] for layer_id in range(num_down_blocks)\n",
    "    }\n",
    "\n",
    "    # Retrieves the keys for the decoder up blocks only\n",
    "    num_up_blocks = len({\".\".join(layer.split(\".\")[:3]) for layer in vae_state_dict if \"decoder.up\" in layer})\n",
    "    up_blocks = {\n",
    "        layer_id: [key for key in vae_state_dict if f\"up.{layer_id}\" in key] for layer_id in range(num_up_blocks)\n",
    "    }\n",
    "\n",
    "    for i in range(num_down_blocks):\n",
    "        resnets = [key for key in down_blocks[i] if f\"down.{i}\" in key and f\"down.{i}.downsample\" not in key]\n",
    "\n",
    "        if f\"encoder.down.{i}.downsample.conv.weight\" in vae_state_dict:\n",
    "            new_checkpoint[f\"encoder.down_blocks.{i}.downsamplers.0.conv.weight\"] = vae_state_dict.pop(\n",
    "                f\"encoder.down.{i}.downsample.conv.weight\"\n",
    "            )\n",
    "            new_checkpoint[f\"encoder.down_blocks.{i}.downsamplers.0.conv.bias\"] = vae_state_dict.pop(\n",
    "                f\"encoder.down.{i}.downsample.conv.bias\"\n",
    "            )\n",
    "\n",
    "        paths = renew_vae_resnet_paths(resnets)\n",
    "        meta_path = {\"old\": f\"down.{i}.block\", \"new\": f\"down_blocks.{i}.resnets\"}\n",
    "        assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n",
    "\n",
    "    mid_resnets = [key for key in vae_state_dict if \"encoder.mid.block\" in key]\n",
    "    num_mid_res_blocks = 2\n",
    "    for i in range(1, num_mid_res_blocks + 1):\n",
    "        resnets = [key for key in mid_resnets if f\"encoder.mid.block_{i}\" in key]\n",
    "\n",
    "        paths = renew_vae_resnet_paths(resnets)\n",
    "        meta_path = {\"old\": f\"mid.block_{i}\", \"new\": f\"mid_block.resnets.{i - 1}\"}\n",
    "        assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n",
    "\n",
    "    mid_attentions = [key for key in vae_state_dict if \"encoder.mid.attn\" in key]\n",
    "    paths = renew_vae_attention_paths(mid_attentions)\n",
    "    meta_path = {\"old\": \"mid.attn_1\", \"new\": \"mid_block.attentions.0\"}\n",
    "    assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n",
    "    conv_attn_to_linear(new_checkpoint)\n",
    "\n",
    "    for i in range(num_up_blocks):\n",
    "        block_id = num_up_blocks - 1 - i\n",
    "        resnets = [\n",
    "            key for key in up_blocks[block_id] if f\"up.{block_id}\" in key and f\"up.{block_id}.upsample\" not in key\n",
    "        ]\n",
    "\n",
    "        if f\"decoder.up.{block_id}.upsample.conv.weight\" in vae_state_dict:\n",
    "            new_checkpoint[f\"decoder.up_blocks.{i}.upsamplers.0.conv.weight\"] = vae_state_dict[\n",
    "                f\"decoder.up.{block_id}.upsample.conv.weight\"\n",
    "            ]\n",
    "            new_checkpoint[f\"decoder.up_blocks.{i}.upsamplers.0.conv.bias\"] = vae_state_dict[\n",
    "                f\"decoder.up.{block_id}.upsample.conv.bias\"\n",
    "            ]\n",
    "\n",
    "        paths = renew_vae_resnet_paths(resnets)\n",
    "        meta_path = {\"old\": f\"up.{block_id}.block\", \"new\": f\"up_blocks.{i}.resnets\"}\n",
    "        assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n",
    "\n",
    "    mid_resnets = [key for key in vae_state_dict if \"decoder.mid.block\" in key]\n",
    "    num_mid_res_blocks = 2\n",
    "    for i in range(1, num_mid_res_blocks + 1):\n",
    "        resnets = [key for key in mid_resnets if f\"decoder.mid.block_{i}\" in key]\n",
    "\n",
    "        paths = renew_vae_resnet_paths(resnets)\n",
    "        meta_path = {\"old\": f\"mid.block_{i}\", \"new\": f\"mid_block.resnets.{i - 1}\"}\n",
    "        assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n",
    "\n",
    "    mid_attentions = [key for key in vae_state_dict if \"decoder.mid.attn\" in key]\n",
    "    paths = renew_vae_attention_paths(mid_attentions)\n",
    "    meta_path = {\"old\": \"mid.attn_1\", \"new\": \"mid_block.attentions.0\"}\n",
    "    assign_to_checkpoint(paths, new_checkpoint, vae_state_dict, additional_replacements=[meta_path], config=config)\n",
    "    conv_attn_to_linear(new_checkpoint)\n",
    "    return new_checkpoint\n",
    "\n",
    "\n",
    "def vae_pt_to_vae_diffuser(\n",
    "    checkpoint_path: str,\n",
    "    output_path: str,\n",
    "    config_json: str,\n",
    "    output_widget,\n",
    "):\n",
    "  try:\n",
    "    # Load the config JSON\n",
    "    with open(config_json, \"r\") as f:\n",
    "        vae_config = json.load(f)\n",
    "\n",
    "    # Load the checkpoint\n",
    "    if checkpoint_path.endswith(\"safetensors\"):\n",
    "        from safetensors import safe_open\n",
    "\n",
    "        checkpoint = {}\n",
    "        with safe_open(checkpoint_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for key in f.keys():\n",
    "                checkpoint[key] = f.get_tensor(key)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "\n",
    "    # Convert the VAE model\n",
    "    converted_vae_checkpoint = custom_convert_ldm_vae_checkpoint(checkpoint, vae_config)\n",
    "\n",
    "    vae = AutoencoderKL(**vae_config)\n",
    "    vae.load_state_dict(converted_vae_checkpoint)\n",
    "\n",
    "    # Save the converted model to a SafeTensors file\n",
    "    with safetensors.safe_open(output_path, \"wb\", framework=\"pt\") as f:\n",
    "        for key, tensor in converted_vae_checkpoint.items():\n",
    "            f.set_tensor(key, tensor.cpu().numpy())\n",
    "\n",
    "    with output_widget:\n",
    "        print(f\"Model converted to SafeTensors format and saved to {output_path}\")\n",
    "\n",
    "  except Exception as e:\n",
    "    with output_widget:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # Create text widgets for input\n",
    "  vae_pt_path_widget = Text(value=\"\", placeholder=\"Path to VAE .pt or .safetensors\", description=\"VAE Path:\")\n",
    "  dump_path_widget = Text(value=\"\", placeholder=\"Path to save the VAE .safetensors\", description=\"Output Path:\")\n",
    "  config_json_widget = Text(value=\"\", placeholder=\"Path to Config Json File\", description=\"Config Json:\")\n",
    "  convert_button = Button(description=\"Convert VAE to Safetensors\")\n",
    "  output_widget = Output()\n",
    "\n",
    "  def handle_conversion(b):\n",
    "    vae_pt_path = vae_pt_path_widget.value\n",
    "    dump_path = dump_path_widget.value\n",
    "    config_json = config_json_widget.value\n",
    "\n",
    "    vae_pt_to_vae_diffuser(vae_pt_path, dump_path, config_json, output_widget)\n",
    "\n",
    "  convert_button.on_click(handle_conversion)\n",
    "\n",
    "  display(vae_pt_path_widget)\n",
    "  display(dump_path_widget)\n",
    "  display(config_json_widget)\n",
    "  display(convert_button)\n",
    "  display(output_widget)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
